# Streams in Node.js

## ðŸ”¹ Introduction

In Node.js, a **Stream** is a way to handle data that is **read or
written continuously**, instead of loading the entire data into memory
at once.

ðŸ‘‰ Think of a stream like water flowing through a pipe:\
- You don't wait for the entire tank to fill before you drink.\
- Instead, you drink as it flows.

------------------------------------------------------------------------

## ðŸ”¹ Why Streams?

-   **Without streams**: Large files (e.g., 1GB) are fully loaded into
    memory â†’ slow & memory heavy.\
-   **With streams**: Data is processed chunk by chunk â†’ faster &
    efficient.

âœ… Benefits: Performance boost & reduced memory usage.

------------------------------------------------------------------------

## ðŸ”¹ Types of Streams

1.  **Readable Streams** â†’ Read data from source.\
    Example: `fs.createReadStream()`\
2.  **Writable Streams** â†’ Write data to destination.\
    Example: `fs.createWriteStream()`\
3.  **Duplex Streams** â†’ Both read & write.\
    Example: `net.Socket`\
4.  **Transform Streams** â†’ Duplex streams that can modify data while
    passing.\
    Example: `zlib.createGzip()`

------------------------------------------------------------------------

## ðŸ”¹ Events in Streams

Streams are `EventEmitter` objects with common events:

-   `data` â†’ when a chunk is available\
-   `end` â†’ when no more data is left\
-   `error` â†’ on errors\
-   `finish` â†’ when writing is done

------------------------------------------------------------------------

## ðŸ”¹ Example: Reading a File

``` js
const fs = require('fs');

const readable = fs.createReadStream('bigfile.txt', { encoding: 'utf8' });

readable.on('data', (chunk) => {
  console.log('Chunk received:', chunk);
});

readable.on('end', () => {
  console.log('Finished reading file.');
});

readable.on('error', (err) => {
  console.error('Error:', err);
});
```

------------------------------------------------------------------------

## ðŸ”¹ Example: Copy File Using Pipe

``` js
const fs = require('fs');

const readable = fs.createReadStream('input.txt');
const writable = fs.createWriteStream('output.txt');

readable.pipe(writable);

console.log("File copied successfully!");
```

------------------------------------------------------------------------

## ðŸ”¹ Real-World Use Cases

-   Handling large files (logs, videos, etc.)\
-   Video/audio streaming (Netflix, YouTube)\
-   Network communication (TCP, HTTP)\
-   Data transformation (compression, encryption)

------------------------------------------------------------------------
# Buffer vs Stream in Node.js

## ðŸ”¹ Buffer
- A **buffer** is a temporary memory that holds binary data.
- In Node.js, buffers are mainly used for handling **binary data** (like files, TCP packets).
- Stores the **entire data** before processing.

### Example
```js
const fs = require('fs');

fs.readFile('bigfile.txt', (err, data) => {
  if (err) throw err;
  console.log("File size:", data.length);
});
âš ï¸ Problem â†’ Loads whole file in memory â†’ slow and memory heavy for large files.

ðŸ”¹ Stream
A stream processes data chunk by chunk as it arrives.

No need to load everything into memory at once.

Efficient for large files, audio/video, and real-time data.

Example
js
Copy code
const fs = require('fs');

const readable = fs.createReadStream('bigfile.txt');

readable.on('data', (chunk) => {
  console.log("Received chunk:", chunk.length);
});
âœ… Only small chunks are stored in memory â†’ low memory usage.

## ðŸ”¹ Major Differences

| Feature         | Buffer                                | Stream                                |
|-----------------|---------------------------------------|---------------------------------------|
| **Data Handling** | Loads entire data at once             | Processes data in chunks              |
| **Memory Usage**  | High (needs full data in memory)      | Low (only handles chunks at a time)   |
| **Performance**   | Slower for large files                | Faster & efficient for large files    |
| **Use Case**      | Small files, binary conversions, crypto | Large files, streaming, network data  |


ðŸ”¹ Why We Use Them
Buffers

For direct binary manipulation (images, encryption, TCP packets).

Suitable for small files where memory isnâ€™t a big concern.

Streams

For large file handling (logs, backups, videos).

For real-time applications (Netflix, YouTube, live chat, HTTP).

âœ… Summary:

Buffer = Store everything in memory first.

Stream = Process data piece by piece while it arrives.

------------------------------------------------------------------------
ðŸ”¹ Do Both Buffer and Stream Transfer Data?

âœ… Yes, both are involved in transferring data from one place to another, but the way they handle it is different.

ðŸ”¹ Buffer

A buffer is like a storage box.

Data is first collected in memory (buffered completely) and then transferred/processed.

Suitable for small data transfers.

ðŸ‘‰ Example:

fs.readFile() â†’ loads the whole file into a buffer, then sends it.

Good for sending small images, configs, JSON files.

ðŸ”¹ Stream

A stream is like a pipeline.

Data is transferred chunk by chunk while still being read/written.

Best for large or continuous transfers.

ðŸ‘‰ Example:

fs.createReadStream() â†’ starts sending data while still reading the file.

Good for video streaming, large backups, live chat, HTTP responses.

ðŸ”¹ Key Idea

Buffer â†’ "Store everything first, then transfer."

Stream â†’ "Transfer as it flows, no need to wait for all data."

âœ… So yes, both are used for data transfer, but the approach differs:

Buffer = batch transfer (all at once).

Stream = continuous transfer (piece by piece)..